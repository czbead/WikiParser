{"paragraphs":[{"text":"%dep\nz.reset()\nz.addRepo(\"Spark Packages Repo\").url(\"http://dl.bintray.com/spark-packages/maven\")\nz.load(\"com.databricks:spark-xml_2.11:0.4.0\")","user":"anonymous","dateUpdated":"2019-09-23T15:08:25-0700","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"res0: org.apache.zeppelin.dep.Dependency = org.apache.zeppelin.dep.Dependency@8ea05da\n"}]},"apps":[],"jobName":"paragraph_1567701441924_1749855958","id":"20190905-093721_10788277","dateCreated":"2019-09-05T09:37:21-0700","dateStarted":"2019-09-23T15:08:25-0700","dateFinished":"2019-09-23T15:08:37-0700","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:409"},{"text":"%spark\nimport org.apache.spark.sql.SparkSession\nimport com.databricks.spark.xml._\n\nval spark = SparkSession.builder.getOrCreate()","user":"anonymous","dateUpdated":"2019-09-23T15:08:29-0700","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"import org.apache.spark.sql.SparkSession\nimport com.databricks.spark.xml._\nspark: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@37e3c0f7\n"}]},"apps":[],"jobName":"paragraph_1567631479662_317190550","id":"20190904-141119_1699504814","dateCreated":"2019-09-04T14:11:19-0700","dateStarted":"2019-09-23T15:08:31-0700","dateFinished":"2019-09-23T15:08:47-0700","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:410"},{"text":"%spark\nval wiki_df = spark.read.format(\"com.databricks.spark.xml\").option(\"rowTag\", \"page\").load(\"hdfs://localhost:9000/data/wiki/wk.xml.bz2\")\n// wiki_df.count\n// wiki_df.printSchema\n// wiki_df.show","user":"anonymous","dateUpdated":"2019-09-23T15:08:42-0700","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"wiki_df: org.apache.spark.sql.DataFrame = [id: bigint, ns: bigint ... 4 more fields]\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://192.168.1.154:4040/jobs/job?id=0"],"interpreterSettingId":"spark"}},"apps":[],"jobName":"paragraph_1567633170875_-436061305","id":"20190904-143930_632322757","dateCreated":"2019-09-04T14:39:30-0700","dateStarted":"2019-09-23T15:08:42-0700","dateFinished":"2019-09-23T15:50:23-0700","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:411"},{"text":"%spark\nwiki_df.count","user":"anonymous","dateUpdated":"2019-09-11T11:23:11-0700","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"res2: Long = 19631176\n"}]},"apps":[],"jobName":"paragraph_1567809128344_-1028822971","id":"20190906-153208_2043156476","dateCreated":"2019-09-06T15:32:08-0700","dateStarted":"2019-09-11T11:23:13-0700","dateFinished":"2019-09-11T11:45:55-0700","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:412"},{"text":"%spark\nval wk_df1 = wiki_df.select($\"id\", $\"ns\", $\"redirect._title\".alias(\"redirect\"), $\"restrictions\", $\"revision\", $\"title\")\n// wk_df1.count\n// wk_df1.printSchema\n// wk_df1.show","user":"anonymous","dateUpdated":"2019-09-23T15:08:57-0700","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"wk_df1: org.apache.spark.sql.DataFrame = [id: bigint, ns: bigint ... 4 more fields]\n"}]},"apps":[],"jobName":"paragraph_1567701382814_1250254318","id":"20190905-093622_1728082884","dateCreated":"2019-09-05T09:36:22-0700","dateStarted":"2019-09-23T15:08:57-0700","dateFinished":"2019-09-23T15:50:24-0700","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:413"},{"text":"%spark\nval wk_df2 = wk_df1.select($\"id\", $\"title\", $\"ns\", $\"redirect\", $\"revision.id\".alias(\"rev_id\"), $\"revision.parentid\".alias(\"rev_parentid\"), $\"revision.text\".alias(\"rev_text\"), $\"revision.contributor\".alias(\"rev_contributor\"), $\"revision.comment\".alias(\"rev_comment\"), $\"revision.timestamp\".alias(\"rev_timestamp\"), $\"revision.format\".alias(\"rev_foramt\"), $\"revision.model\".alias(\"rev_model\"), $\"revision.minor\".alias(\"rev_minor\"), $\"revision.sha1\".alias(\"rev_sha1\"), $\"restrictions\")\n// wk_df2.count\n// wk_df2.printSchema\n// wk_df2.show","user":"anonymous","dateUpdated":"2019-09-23T15:09:06-0700","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"wk_df2: org.apache.spark.sql.DataFrame = [id: bigint, title: string ... 13 more fields]\n"}]},"apps":[],"jobName":"paragraph_1567704704356_705746751","id":"20190905-103144_1195147529","dateCreated":"2019-09-05T10:31:44-0700","dateStarted":"2019-09-23T15:50:24-0700","dateFinished":"2019-09-23T15:50:24-0700","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:414"},{"text":"%spark\nval wk_df3 = wk_df2.select($\"id\", $\"title\", $\"ns\", $\"redirect\", $\"rev_id\", $\"rev_parentid\", $\"rev_text\", $\"rev_contributor\", $\"rev_comment._VALUE\".alias(\"rev_comment\"), $\"rev_comment._deleted\".alias(\"rev_comment_status\"), $\"rev_timestamp\", $\"rev_foramt\", $\"rev_model\", $\"rev_minor\", $\"rev_sha1\", $\"restrictions\")\n// wk_df3.count\n// wk_df3.printSchema\n// wk_df3.show","user":"anonymous","dateUpdated":"2019-09-23T15:09:16-0700","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"wk_df3: org.apache.spark.sql.DataFrame = [id: bigint, title: string ... 14 more fields]\n"}]},"apps":[],"jobName":"paragraph_1567704716907_-193718774","id":"20190905-103156_1510932952","dateCreated":"2019-09-05T10:31:56-0700","dateStarted":"2019-09-23T15:50:24-0700","dateFinished":"2019-09-23T15:50:25-0700","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:415"},{"text":"%spark\nval wk_df4 = wk_df3.select($\"id\", $\"title\", $\"ns\", $\"redirect\", $\"rev_id\", $\"rev_parentid\".alias(\"rev_pid\"), $\"rev_text\", $\"rev_contributor.id\".alias(\"rev_ctr_id\"), $\"rev_contributor.ip\".alias(\"rev_ctr_ip\"), $\"rev_contributor.username\".alias(\"rev_ctr_user\"), $\"rev_contributor._deleted\".alias(\"rev_ctr_st\"), $\"rev_comment\".alias(\"rev_cmt\"), $\"rev_comment_status\".alias(\"rev_cmt_st\"), $\"rev_timestamp\".alias(\"rev_ts\"), $\"rev_foramt\", $\"rev_model\", $\"rev_sha1\", $\"restrictions\".alias(\"res\"))\n// wk_df4.count\n// wk_df4.printSchema\n// wk_df4.show","user":"anonymous","dateUpdated":"2019-09-23T15:09:23-0700","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"wk_df4: org.apache.spark.sql.DataFrame = [id: bigint, title: string ... 16 more fields]\n"}]},"apps":[],"jobName":"paragraph_1567704725443_715539466","id":"20190905-103205_1544141714","dateCreated":"2019-09-05T10:32:05-0700","dateStarted":"2019-09-23T15:50:25-0700","dateFinished":"2019-09-23T15:50:25-0700","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:416"},{"text":"%spark\nval wk_df5 = wk_df4.select($\"id\", $\"title\", $\"ns\", $\"redirect\", $\"rev_id\", $\"rev_pid\", $\"rev_text._VALUE\".alias(\"rev_text\"), $\"rev_ctr_id\", $\"rev_ctr_ip\", $\"rev_ctr_user\", $\"rev_ctr_st\", $\"rev_cmt\", $\"rev_cmt_st\", $\"rev_ts\", $\"rev_foramt\", $\"rev_model\", $\"rev_sha1\", $\"res\")\n// wk_df5.count\n// wk_df5.printSchema\n// wk_df5.show","user":"anonymous","dateUpdated":"2019-09-23T15:09:31-0700","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"wk_df5: org.apache.spark.sql.DataFrame = [id: bigint, title: string ... 16 more fields]\n"}]},"apps":[],"jobName":"paragraph_1567704778436_-723844443","id":"20190905-103258_1360732139","dateCreated":"2019-09-05T10:32:58-0700","dateStarted":"2019-09-23T15:50:25-0700","dateFinished":"2019-09-23T15:50:26-0700","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:417"},{"text":"%spark\nval wk_text = wk_df5.select($\"id\", $\"rev_id\", $\"rev_text\")\n// wk_text.count\n// wk_text.printSchema\n// wk_text.show","user":"anonymous","dateUpdated":"2019-09-23T15:09:40-0700","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"wk_text: org.apache.spark.sql.DataFrame = [id: bigint, rev_id: bigint ... 1 more field]\n"}]},"apps":[],"jobName":"paragraph_1567704780427_677129636","id":"20190905-103300_620078155","dateCreated":"2019-09-05T10:33:00-0700","dateStarted":"2019-09-23T15:50:26-0700","dateFinished":"2019-09-23T15:50:26-0700","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:418"},{"text":"%spark\nval wk_xml = wk_df5","user":"anonymous","dateUpdated":"2019-09-23T15:09:46-0700","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"wk_xml: org.apache.spark.sql.DataFrame = [id: bigint, title: string ... 16 more fields]\n"}]},"apps":[],"jobName":"paragraph_1568949383147_-410903890","id":"20190919-201623_444152607","dateCreated":"2019-09-19T20:16:23-0700","dateStarted":"2019-09-23T15:50:26-0700","dateFinished":"2019-09-23T15:50:26-0700","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:419"},{"text":"%spark\nwk_text.write.mode(\"overwrite\").parquet(\"hdfs://localhost:9000/data/wiki/wk_text_parquet/\")","user":"anonymous","dateUpdated":"2019-09-17T10:14:44-0700","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"org.apache.spark.SparkException: Job aborted.\n  at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:224)\n  at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:154)\n  at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:104)\n  at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:102)\n  at org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:122)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n  at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n  at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:80)\n  at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:80)\n  at org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:656)\n  at org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:656)\n  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:77)\n  at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:656)\n  at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:273)\n  at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:267)\n  at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:225)\n  at org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:549)\n  ... 67 elided\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 163.0 failed 1 times, most recent failure: Lost task 1.0 in stage 163.0 (TID 7540, localhost, executor driver): org.apache.spark.SparkException: Task failed while writing rows.\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:285)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:197)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:196)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.lang.OutOfMemoryError: GC overhead limit exceeded\n\nDriver stacktrace:\n  at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1661)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1649)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1648)\n  at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n  at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1648)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n  at scala.Option.foreach(Option.scala:257)\n  at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1882)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1831)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1820)\n  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n  at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2034)\n  at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:194)\n  ... 87 more\nCaused by: org.apache.spark.SparkException: Task failed while writing rows.\n  at org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:285)\n  at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:197)\n  at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:196)\n  at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n  at org.apache.spark.scheduler.Task.run(Task.scala:109)\n  at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n  ... 3 more\nCaused by: java.lang.OutOfMemoryError: GC overhead limit exceeded\n"}]},"apps":[],"jobName":"paragraph_1567704799108_-1475534238","id":"20190905-103319_2077011075","dateCreated":"2019-09-05T10:33:19-0700","dateStarted":"2019-09-17T10:14:44-0700","dateFinished":"2019-09-17T10:17:36-0700","status":"ERROR","progressUpdateIntervalMs":500,"$$hashKey":"object:420"},{"text":"%spark\nwk_df5.write.mode(\"overwrite\").parquet(\"hdfs://localhost:9000/data/wiki/wk_xml_parquet/\")","user":"anonymous","dateUpdated":"2019-09-11T18:13:10-0700","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"org.apache.spark.SparkException: Job aborted.\n  at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:224)\n  at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:154)\n  at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:104)\n  at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:102)\n  at org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:122)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n  at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n  at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:80)\n  at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:80)\n  at org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:656)\n  at org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:656)\n  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:77)\n  at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:656)\n  at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:273)\n  at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:267)\n  at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:225)\n  at org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:549)\n  ... 49 elided\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 7 in stage 26.0 failed 1 times, most recent failure: Lost task 7.0 in stage 26.0 (TID 1211, localhost, executor driver): org.apache.spark.SparkException: Task failed while writing rows.\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:285)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:197)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:196)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.lang.OutOfMemoryError: GC overhead limit exceeded\n\nDriver stacktrace:\n  at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1661)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1649)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1648)\n  at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n  at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1648)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n  at scala.Option.foreach(Option.scala:257)\n  at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1882)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1831)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1820)\n  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n  at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2034)\n  at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:194)\n  ... 69 more\nCaused by: org.apache.spark.SparkException: Task failed while writing rows.\n  at org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:285)\n  at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:197)\n  at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:196)\n  at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n  at org.apache.spark.scheduler.Task.run(Task.scala:109)\n  at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n  ... 3 more\nCaused by: java.lang.OutOfMemoryError: GC overhead limit exceeded\n"}]},"apps":[],"jobName":"paragraph_1568175763548_825064752","id":"20190910-212243_121678454","dateCreated":"2019-09-10T21:22:43-0700","dateStarted":"2019-09-11T07:54:49-0700","dateFinished":"2019-09-11T07:56:44-0700","status":"ERROR","progressUpdateIntervalMs":500,"$$hashKey":"object:421"},{"text":"%spark\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n","user":"anonymous","dateUpdated":"2019-09-05T10:33:17-0700","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1567704790708_1006393586","id":"20190905-103310_2029961906","dateCreated":"2019-09-05T10:33:10-0700","status":"READY","progressUpdateIntervalMs":500,"$$hashKey":"object:422"}],"name":"/WKP/Parser/STEP1: XML Parsing","id":"2EPS434UQ","noteParams":{},"noteForms":{},"angularObjects":{"spark:shared_process":[]},"config":{"isZeppelinNotebookCronEnable":false,"looknfeel":"default","personalizedMode":"false"},"info":{}}